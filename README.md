# Explaining Facial Emotion Recognition with Grad-CAM ğŸ”
[Link to Web App](https://khaeuss808-xai-finalproject-app-bag1kq.streamlit.app/)

This repository contains the code for my final project for **AIPI 590 â€“ Explainable AI** with **Dr. Bent** at **Duke University**.

The project explores how **Grad-CAM** can be used to visualize what a **ResNet50-based emotion recognition model** is actually looking at when making predictions, and how these explanations can support **trust, debugging, and fairness analysis** in high-stakes applications.

---

## ğŸ¯ Project Motivation

Emotion recognition AI is increasingly deployed in **hiring, healthcare, education, and law enforcement**â€”domains where misclassification can have serious consequences. Despite this, most models are **black boxes**: they output labels like â€œangryâ€ or â€œfearfulâ€ without any visibility into *why*.

Even state-of-the-art systems (e.g., Metaâ€™s DeepFace) achieve high accuracy, but their internal decision making is opaque and often proprietary. When a model says someone is angry, did it focus on the eyebrows? The mouth? Background artifacts? We usually donâ€™t know.

This project aims to:

- **Build trust through transparency**  
  Use Grad-CAM to visualize where the model is â€œlookingâ€ on the face.

- **Detect potential biases**  
  See which features the model relies on and whether they make sense.

- **Debug misclassifications**  
  Identify when predictions are driven by irrelevant regions (e.g., hair, background).

- **Support fairness auditing**  
  Provide a foundation for future work that examines performance across demographic groups.

The accompanying webapp allows users to upload or select images, run the emotion classifier, and view Grad-CAM heatmaps overlaid on the original faces.

---

## ğŸ›ï¸ Model Architecture

The core model is a **ResNet50** classifier fine-tuned for facial emotion recognition.

**Base Architecture**

- Backbone: ResNet50 (50 layers deep)
- Pre-trained on ImageNet (~1.2M images)
- ~23.5M parameters

**Modifications**

- Replaced the final classification layer
- 7 output classes (emotions)
- Fine-tuned on the RAF-DB dataset

**Training Details**

- Optimizer: Adam
- Learning rate: 0.0001 (reduced for fine-tuning)
- Epochs: 15
- Batch size: 32
- Input size: 256Ã—256 (upscaled from 100Ã—100)

**Why ResNet50?**

- Deep architecture captures subtle facial features
- Transfer learning from ImageNet provides strong feature extraction
I experimented with other models, including developing my own CNN from scratch, but found ResNet50 yielded the strongest performance as it already had robust feature extraction capabilities for things like eyes and noses, whereas my CNN struggled and took much longer to learn these features meaningfully. Models like Meta's DeepFace are state of the art in this space, but its architecture is not publicly available, making applying easily interpretable methods like GradCAM difficult.

Models like **Metaâ€™s DeepFace** may be more advanced, but their closed architectures make them harder to interpret using methods like Grad-CAM. ResNet50 strikes a good balance between performance and interpretability.

---

## ğŸ“š Dataset: RAF-DB (Real-world Affective Faces Database)

**Dataset Characteristics**

- Training set: 12,271 images
- Test set: ~3,000 images
- Image size: 100Ã—100 pixels (face-cropped)
- Source: In-the-wild internet images (varied lighting, pose, occlusions)

**Emotion Categories (7)**

1. ğŸ˜Š Happy  
2. ğŸ˜¢ Sad  
3. ğŸ˜  Angry  
4. ğŸ˜® Surprised  
5. ğŸ˜¨ Fearful  
6. ğŸ¤¢ Disgusted  
7. ğŸ˜ Neutral  

**Why This Dataset Was Selected:**

- Each image labeled by ~40 independent annotators, so crowdsourced emotion labeling ensures a level of reliability.
- Captures real-world expression variability: lighting, poses, obstructions (glasses, hair, hands), wide range of gender, ages and races.
- Images are in color which I thought would be necessary for future steps of the project where I might explore skin tone biases.

**Citation**

> Li, S., & Deng, W. (2019). Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition. *IEEE Transactions on Image Processing*.

---

## ğŸ“ˆ Model Performance

**Overall Test Accuracy:** **82.0%**

**Best-Performing Class**

- ğŸŸ¢ **Happy** â€“ **92.7%**

**Most Challenging Class**

- ğŸ”´ **Fearful** â€“ **55.4%**

**Accuracy by Emotion**

- ğŸŸ¢ **Happy:** 92.7%  
- ğŸŸ¢ **Sad:** 83.5%  
- ğŸŸ¢ **Surprised:** 83.0%  
- ğŸŸ¡ **Angry:** 74.7%  
- ğŸŸ¡ **Neutral:** 70.1%  
- ğŸŸ¡ **Disgusted:** 66.9%  
- ğŸ”´ **Fearful:** 55.4%  

These results highlight that the model is quite strong on more prototypical expressions (e.g., happiness) and struggles more with subtle or easily confused emotions like fear or disgustâ€”making explainability particularly important when interpreting predictions.

---

## ğŸ” Explainability: Grad-CAM

**What is Grad-CAM?**
Grad-CAM is a visualization technique that highlights which regions of an image were important for a model's prediction. It was one of my favorite techniques we learned this semester, and I knew I wanted to incorporate it into my final project.

**Why Grad-CAM for Emotion Recognition?**

- Reveals which facial features drive predictions (eyes, mouth, eyebrows)
- Can clearly diagnose diagnose model failures (scattered attention = uncertainty)
- Validates that model uses human-interpretable features
- Builds trust by making "black box" decisions transparent

---

## ğŸ§­ Future Work: Fairness & Demographic Analysis  
Originally, I had intended the focus of this project to be on the differences in accuracy of emotion detection models across different demographic groups (racial, gender, age), inspired in part by my reading of "Unmasking AI" by Joy Buolamwini this semester. However, as I dove deeper into this project, I encountered challenges in finding datasets with the kind of demographic annotations needed to perform the analysis I was interested in. Many popular emotion recognition datasets I considered lack detailed demographic information (FER-2013, RAF-DB), or the demographic datasets I found lacked emotion labeling (UTKFace, FairFace). While completing this project, I considered doing the manual labeling of demographics on the RAF-DB dataset, but I thought this would be innaccurate as race and gender was often hard for me to discern and could lead to me injecting my own implicit biases into the labeling. In the future, expanding this project will require accessing or building a dataset with reliable demographic annotations, enabling a more rigorous investigation into model performance disparities.  

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+  
- `pip` or `conda` for dependency management  
- (Optional) GPU support for faster inference

### Installation

```bash
# Clone the repository
git clone https://github.com/khaeuss808/XAI_FinalProject.git
cd XAI_FinalProject

# Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt